# ![picture](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Hadoop_logo.svg/1280px-Hadoop_logo.svg.png)

>Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.

Three components of hadoop are : 

1. Storage unit : 
    * Hdfs
    * Deafult size of a block is 128 mb 
    * HDFS makeg copieg of the data and storage it across multiple system 
2. MapReduce
    
    ![picture alt]( https://clojurebridgelondon.github.io/workshop/images/map-reduce-sandwich.png "Title is optional")

3. Yarn 
    * It consist of :
        * Resourse manager
        * Node Manager 
        * Application Master
        * Containers 
    